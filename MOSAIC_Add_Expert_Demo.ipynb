{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6882e791-03b2-44f9-b1d3-b4fb6cae7ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2285 classes predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_896400/3689443254.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filename))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('MOSAIC/PredictionUtils') # This is where the library is located\n",
    "\n",
    "from Transformation_Model import KernelMetricNetwork\n",
    "from ChemUtils import create_rxn_Mix_FP\n",
    "\n",
    "def load_model(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model      \n",
    "\n",
    "KMN_cpth = 'best_model_50ep_4096batchsize_AdamW.pth'\n",
    "\n",
    "model = load_model(KernelMetricNetwork(2048*3, 2285), KMN_cpth)\n",
    "model.eval();  # Set to evaluation mode\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device);\n",
    "\n",
    "features = pickle.load('Features.pkl') # This is the set of all features already processed by KMN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005a26ad-359f-4160-b7cc-61da9494e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Overwriting variable to make it smaller for this test.\n",
    "features = features[:1000] # Top-1000 extracted features from KMN. They were previously computed in the process of building FAISS index for MOSAIC.\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb88608-7741-456f-acf0-67c0c8f7dda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original index...\n",
      "Feature dimension: 128\n",
      "Sampled 500 new features\n",
      "Building Level 2 index with 5 clusters...\n",
      "Training Level 2 index...\n",
      "Adding features to Level 2...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from faiss import read_index\n",
    "\n",
    "# ============= LOAD ORIGINAL INDEX AND DATA =============\n",
    "print('Loading original index...')\n",
    "index = read_index(\"RSFP_Index.index\") # testing reading the file\n",
    "res = faiss.StandardGpuResources()\n",
    "index_level1 = faiss.index_cpu_to_gpu(res, 0, index_level1)\n",
    "\n",
    "# Extract feature dimension from index\n",
    "d = index_level1.d\n",
    "print(f'Feature dimension: {d}')\n",
    "\n",
    "# ============= RANDOMLY SAMPLE NEW FEATURES (Level 2, the new data that needs to be incrementally added) =============\n",
    "np.random.seed(2)\n",
    "sample_indices = np.random.choice(len(features), size=500, replace=False)\n",
    "\n",
    "new_features = np.array([features[i] for i in sample_indices])\n",
    "\n",
    "print(f'Sampled {len(new_features)} new features')\n",
    "\n",
    "# ============= BUILD LEVEL 2 INDEX =============\n",
    "nlist_new = 5  # Creating 5 clusters for demonstration\n",
    "print(f'Building Level 2 index with {nlist_new} clusters...')\n",
    "\n",
    "quantizer_level2 = faiss.IndexFlatL2(d)\n",
    "index_level2 = faiss.IndexIVFFlat(quantizer_level2, d, nlist_new)\n",
    "res2 = faiss.StandardGpuResources()\n",
    "index_level2 = faiss.index_cpu_to_gpu(res2, 0, index_level2)\n",
    "\n",
    "print('Training Level 2 index...')\n",
    "index_level2.train(new_features)\n",
    "assert index_level2.is_trained\n",
    "\n",
    "print('Adding features to Level 2...')\n",
    "index_level2.add(new_features)\n",
    "\n",
    "# ============= HIERARCHICAL SEARCH FUNCTION =============\n",
    "def hierarchical_search(query_feat, k=3): # Default searching with nearest 3 Voronoi reagions (k=3)\n",
    "    # Search Level 1\n",
    "    D1, I1 = index_level1.quantizer.search(query_feat.reshape(1, -1), k)\n",
    "    \n",
    "    # Search Level 2\n",
    "    D2, I2 = index_level2.quantizer.search(query_feat.reshape(1, -1), k)\n",
    "    \n",
    "    # Concatenate and rank\n",
    "    all_distances = np.concatenate([D1[0], D2[0]])\n",
    "    all_indices = np.concatenate([\n",
    "        I1[0], \n",
    "        I2[0] + index_level1.ntotal  # Offset by Level 1 size. If this one indeed gets returned, the indices will not overlap.\n",
    "    ])\n",
    "    \n",
    "    sorted_idx = np.argsort(all_distances) # Combining results from hierarchical search\n",
    "    return all_distances[sorted_idx][:k], all_indices[sorted_idx][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a610acb-5c3e-45d0-a071-9a2dcc93176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: BrC1=CC=CC=C1.C2COCCN2>>C3(N4CCOCC4)=CC=CC=C3\n",
      "\n",
      "============================================================\n",
      "LEVEL 1 RESULTS (Original Index)\n",
      "============================================================\n",
      "  1. Expert   59 | Distance: 29.32\n",
      "  2. Expert 1021 | Distance: 43.02\n",
      "  3. Expert 1774 | Distance: 48.19\n",
      "\n",
      "============================================================\n",
      "LEVEL 2 RESULTS (New Index)\n",
      "============================================================\n",
      "  1. Expert    0 | Distance: 308.35\n",
      "  2. Expert    2 | Distance: 397.04\n",
      "  3. Expert    3 | Distance: 459.27\n",
      "\n",
      "============================================================\n",
      "HIERARCHICAL RESULTS (Combined & Ranked)\n",
      "============================================================\n",
      "  1. Distance: 29.32 | Level 1 Expert 59\n",
      "  2. Distance: 43.02 | Level 1 Expert 1021\n",
      "  3. Distance: 48.19 | Level 1 Expert 1774\n"
     ]
    }
   ],
   "source": [
    "fp_size = 1024\n",
    "clean_reactions = [\n",
    "    'BrC1=CC=CC=C1.C2COCCN2>>C3(N4CCOCC4)=CC=CC=C3', # Classic Buchwald-Hartwig\n",
    "    'BrC1=CC=CC=C1.CC2(C)C(C)(C)OB(B3OC(C)(C)C(C)(C)O3)O2>>CC(O4)(C)C(C)(C)OB4C5=CC=CC=C5', # Suzuki\n",
    "]\n",
    "\n",
    "test_features = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(clean_reactions)):\n",
    "        rxn = clean_reactions[i]\n",
    "        rxn_fp = create_rxn_Mix_FP(rxn, rxnfpsize=fp_size, pfpsize=fp_size, useChirality=True) \n",
    "        rxn_fp = np.concatenate((rxn_fp[1],rxn_fp[2],rxn_fp[0]), axis = -1) # reactant, diff, product\n",
    "        feat = model.get_embeddings(torch.from_numpy(rxn_fp).view(1,-1).float().to(device))\n",
    "        test_features.append(feat.cpu().numpy())\n",
    "        \n",
    "test_feats = np.array(test_features).squeeze()\n",
    "\n",
    "# Query\n",
    "query_rxn = clean_reactions[0]\n",
    "query_feat = test_feats[0]\n",
    "\n",
    "# Search both levels independently\n",
    "k = 3\n",
    "D1, I1 = index_level1.quantizer.search(query_feat.reshape(1, -1), k)\n",
    "D2, I2 = index_level2.quantizer.search(query_feat.reshape(1, -1), k)\n",
    "\n",
    "print(f'\\nQuery: {query_rxn}')\n",
    "print('\\n' + '='*60)\n",
    "print('LEVEL 1 RESULTS (Original Index)')\n",
    "print('='*60)\n",
    "for i, (dist, idx) in enumerate(zip(D1[0], I1[0])):\n",
    "    print(f'  {i+1}. Expert {idx:4d} | Distance: {dist:.2f}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('LEVEL 2 RESULTS (New Index)')\n",
    "print('='*60)\n",
    "for i, (dist, idx) in enumerate(zip(D2[0], I2[0])):\n",
    "    print(f'  {i+1}. Expert {idx:4d} | Distance: {dist:.2f}')\n",
    "\n",
    "# Hierarchical search (concatenated, ranked, and used as one framework).\n",
    "distances, indices = hierarchical_search(query_feat, k=k)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('HIERARCHICAL RESULTS (Combined & Ranked)')\n",
    "print('='*60)\n",
    "for i, (dist, idx) in enumerate(zip(distances, indices)):\n",
    "    level = \"Level 1\" if idx < index_level1.ntotal else \"Level 2\"\n",
    "    actual_idx = idx if idx < index_level1.ntotal else idx - index_level1.ntotal\n",
    "    print(f'  {i+1}. Distance: {dist:.2f} | {level} Expert {actual_idx}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMS_Env",
   "language": "python",
   "name": "paroute_cross_referencing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
